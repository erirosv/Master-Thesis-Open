# Master Thesis Open

**IMPORTANT!** A part of the code used for this Master  Thesis is proprietary, hence you need to write it your self.

## Overview
This project is benchmarking different classifiactions algorithms agains the SPFSR
on microarray data. The data is high dimentional, high amount of columns and low
amount of rows. All the data is open sourced. 

## Structure of the thesis
The structure is the classic one. An overview of the structure:
1. Introduction
2. Theory
3. Methods
4. Results
5. Conclusion

## Data
The table does not show all the colums.

| Name | Instances | Features | Classes |
| :--------: | :--------: | :--------: | :--------: |
| Alon   | 62    | 2000    | 2   |
| Borovecki    | 31   | 22283    | 2    |
| Burcyznski      | 127      | 22283     | 3      |
| Christensen   | 217  | 1413   | 2   |
| Chin    | 118    | 22215    | 2    |
| Chowdary    | 104    | 22283    | 2    |
| Chiaretti    | 31    | 12625    | 2    |
| Gordon    | 181    | 12533    | 2    |
| Golub    | 72    | 7129    | 2    |
| Gravier    | 168    | 2905    | 2    |
| Khan    | 63    | 2308    | 4    |
| Nakayama    | 105    | 22283    | 10    |
| Pomeroy    | 60    | 7128    | 2    |
| Shipp    | 77    | 6817    | 2    |
| Singh    | 102    | 12600    | 2    |
| Sorlie    | 85    | 456    | 5    |
| Su    | 102    | 5565    | 4    |
| Subramanian    | 50   | 10100    | 2    |
| Sun    | 102    | 54613    | 4    |
| Tian    | 173    | 12625    | 2    |
| West    | 49    | 7129    | 2    |
| Yeoh    | 248    | 12625    | 6    |


## Methods
The algorithm used are mainly using wrappers and filters, there can also be some hybridsd and embedded.

| Acronym | Method |
| :--------: | :--------: |
| CFS | Correlation-Based Feature Selection |
| FScore | FScore |
| GA | Genetic Algorithm |
| InfoGain | Information Gain |
| MRMR | Minimum Redundancy Maximum Relevance |
| RFI | residual feed intake |
| ReliefF | Relief-Based Feature Selection |
| SFS | Sequential Forward Selection |
| SPFSR | Simultaneous Perturbation Stochastic Approximation for feature selection and ranking |

## Algorithms
The algorithms used are the following:
- Na√Øve Bayes
- K-Nearest Neighbours
- Decision Tree
- Support Vector Machine


